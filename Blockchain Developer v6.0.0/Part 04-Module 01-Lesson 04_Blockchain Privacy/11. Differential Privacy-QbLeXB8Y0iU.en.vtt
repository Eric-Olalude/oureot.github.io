WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.009
In this section, we'll go over another privacy technique known as differential privacy.

00:00:05.009 --> 00:00:07.349
We'll focus on what it is,

00:00:07.349 --> 00:00:09.285
who is using it, how it works,

00:00:09.285 --> 00:00:11.384
and what the future for this technique is.

00:00:11.384 --> 00:00:13.559
Companies want access to our data to help

00:00:13.560 --> 00:00:16.170
improve performance and usability of their products.

00:00:16.170 --> 00:00:19.590
On the other hand, as users of these products we expect

00:00:19.589 --> 00:00:23.339
privacy and don't want companies to be able to track the things we do.

00:00:23.339 --> 00:00:26.100
This is a problem that many companies face and it could be

00:00:26.100 --> 00:00:29.340
solved using a technique known as differential privacy.

00:00:29.339 --> 00:00:33.890
Differential privacy is a technique used to maximize both the usefulness of

00:00:33.890 --> 00:00:38.649
a set of data as well as the privacy of the individuals within the dataset.

00:00:38.649 --> 00:00:42.469
It allows us to gain important insights about a group whose data has been

00:00:42.469 --> 00:00:46.744
collected without revealing the details about specific members of that group.

00:00:46.744 --> 00:00:49.204
This is important for a few reasons.

00:00:49.204 --> 00:00:52.070
To learn why, let's go over a few examples of how

00:00:52.070 --> 00:00:55.145
centralized companies use this including Google Maps,

00:00:55.145 --> 00:00:57.515
iPhone keyboard, and Google Chrome.

00:00:57.515 --> 00:01:01.804
The common theme between each of these examples is that companies want to learn

00:01:01.804 --> 00:01:03.500
general patterns of behavior from

00:01:03.500 --> 00:01:07.534
its users without being able to trace data to a single user.

00:01:07.534 --> 00:01:13.369
Afterwards, we'll go over how this could be transferred to a decentralized environment.

00:01:13.370 --> 00:01:16.010
Google Maps allows you to search any location in

00:01:16.010 --> 00:01:18.995
the world and helps you navigate using GPS.

00:01:18.995 --> 00:01:21.295
By using GPS and Google Maps,

00:01:21.295 --> 00:01:24.305
you're creating a lot of data about all the places you travel,

00:01:24.305 --> 00:01:27.290
but you wouldn't want Google to be able to look into this data to

00:01:27.290 --> 00:01:30.365
study your specific individual traveling habits.

00:01:30.364 --> 00:01:33.349
On the other hand, Google can use this data to improve

00:01:33.349 --> 00:01:37.189
its service as important that they have some kind of access to it.

00:01:37.189 --> 00:01:41.730
Using differential privacy, the data they collect can't be traced back to you

00:01:41.730 --> 00:01:44.329
specifically but can be aggregated into

00:01:44.329 --> 00:01:48.564
a larger collection of data to analyze the general habits of all of its users.

00:01:48.564 --> 00:01:53.584
This allows them to do things like analyse traffic patterns based on the time of day.

00:01:53.584 --> 00:01:58.074
The same ideas are used by other Google products like Google Chrome.

00:01:58.075 --> 00:02:01.885
For example, they could analyse trends of web browsing behavior

00:02:01.885 --> 00:02:06.100
without being able to access your specific personal search history.

00:02:06.099 --> 00:02:09.264
Other companies have taken the step to ensure privacy.

00:02:09.264 --> 00:02:11.949
One example is the Apple iPhone keyboard.

00:02:11.949 --> 00:02:13.869
By using the keyboard again,

00:02:13.870 --> 00:02:17.094
you're generating a lot of private data that could be traced back to you.

00:02:17.094 --> 00:02:20.784
But Apple can use this data in its applications for things like

00:02:20.784 --> 00:02:24.924
determining which word uses a typing that aren't currently in their dictionary.

00:02:24.925 --> 00:02:28.985
Reading of messages word for word would definitely be an invasion of your privacy.

00:02:28.985 --> 00:02:34.405
So, Apple has taken steps to ensure that this doesn't happen using differential privacy.

00:02:34.405 --> 00:02:36.835
As a side note, you might be thinking,

00:02:36.835 --> 00:02:39.010
can't we just anonymize the dataset by

00:02:39.009 --> 00:02:41.870
removing our names and other identifying information.

00:02:41.870 --> 00:02:45.365
As it turns out, this approach can be less effective than it seems.

00:02:45.365 --> 00:02:51.020
For one, anonymous data is often either not actually anonymous or not that valuable.

00:02:51.020 --> 00:02:53.780
Using techniques like linkage attacks,

00:02:53.780 --> 00:02:56.930
data analysts can actually use the data from various sources to

00:02:56.930 --> 00:03:01.040
infer the identity of users within a set of anonymous data,

00:03:01.039 --> 00:03:05.060
and they can get extremely accurate results about which data came from you.

00:03:05.060 --> 00:03:06.979
On the other end of the spectrum,

00:03:06.979 --> 00:03:08.224
if you were just strip the data of

00:03:08.224 --> 00:03:11.674
all the qualities needed to be truly completely anonymous,

00:03:11.675 --> 00:03:16.145
you'd likely end up with a dataset that you can't derive any valuable insights from.

00:03:16.145 --> 00:03:20.870
For these reasons, anonymizing or de-identifying data is not always an option,

00:03:20.870 --> 00:03:23.974
that brings us back to the importance of differential privacy.

00:03:23.974 --> 00:03:25.669
So, how is this any different?

00:03:25.669 --> 00:03:29.824
How does it help to gain insights without intruding into users privacy?

00:03:29.824 --> 00:03:33.334
Well, to do this, the companies collecting data apply

00:03:33.335 --> 00:03:38.034
differential privacy techniques such as random noise to the data they're collecting.

00:03:38.034 --> 00:03:41.900
Rather than get the fully accurate data about each individual user,

00:03:41.900 --> 00:03:44.120
the companies receive information that can't

00:03:44.120 --> 00:03:46.745
be traced back to the user they are collected from.

00:03:46.745 --> 00:03:49.640
Here's an example using a simple survey.

00:03:49.639 --> 00:03:54.799
Let's say we asked you and 1,000 other people if they enjoy their current job,

00:03:54.800 --> 00:03:58.040
and we allow them to answer yes or no to the question.

00:03:58.039 --> 00:04:01.924
Using differential privacy, we can't just collect each of these answers.

00:04:01.925 --> 00:04:05.844
We have to use an algorithm that introduces some noise to the answers.

00:04:05.844 --> 00:04:07.520
Here's a simple example.

00:04:07.520 --> 00:04:11.284
To start, the algorithm takes your answer and flips a coin.

00:04:11.284 --> 00:04:12.979
If the coin flip is heads,

00:04:12.979 --> 00:04:14.629
your real answer is sent through,

00:04:14.629 --> 00:04:16.370
if the coin flip is tails,

00:04:16.370 --> 00:04:20.930
the algorithm will send a random answer derived from flipping coin a second time.

00:04:20.930 --> 00:04:22.790
Using the second flip,

00:04:22.790 --> 00:04:27.245
the algorithm will send yes if it flips heads and no if it flips tails.

00:04:27.245 --> 00:04:28.759
When we look at the data,

00:04:28.759 --> 00:04:31.024
the noise we've introduced makes it impossible for

00:04:31.024 --> 00:04:33.949
us to fully trust the data for any specific user,

00:04:33.949 --> 00:04:37.430
that is, each user now has plausible deniability.

00:04:37.430 --> 00:04:40.490
So, when looking at one user's data,

00:04:40.490 --> 00:04:44.555
there's a 50 percent chance that the answer is based on that user's true response,

00:04:44.555 --> 00:04:48.264
and a 50 percent chance that answer is based on a random coin flip.

00:04:48.264 --> 00:04:53.419
Also, since the data analyst knows how much of the data is due to this random noise,

00:04:53.420 --> 00:04:55.730
they can compensate for it and extrapolate

00:04:55.730 --> 00:04:59.645
an accurate view of the population of people who responded to their server.

00:04:59.644 --> 00:05:02.539
This example is simple compared to the types of

00:05:02.540 --> 00:05:06.620
distributions and randomization that happen in real differential privacy.

00:05:06.620 --> 00:05:08.975
But this basic idea still holds true.

00:05:08.975 --> 00:05:11.689
There are many benefits of differential privacy,

00:05:11.689 --> 00:05:13.625
but there are some complications too.

00:05:13.625 --> 00:05:17.089
For one, the noise introduced in the dataset requires

00:05:17.089 --> 00:05:21.394
the dataset to be large in order to compensate for the misinformation within it.

00:05:21.394 --> 00:05:24.649
It's also a bit more difficult to implement because of

00:05:24.649 --> 00:05:28.745
the extra steps involved to ensure that protected answers don't leak out somehow.

00:05:28.745 --> 00:05:32.269
That said, privacy is important and this effort continues

00:05:32.269 --> 00:05:35.944
to move forward in both centralized and decentralized applications.

00:05:35.944 --> 00:05:39.800
It's a privacy technique that allows data analysts to gain insights from

00:05:39.800 --> 00:05:44.525
information all while allowing the information of an individual user to stay private.

00:05:44.524 --> 00:05:48.739
It does this by injecting random noise into the dataset.

00:05:48.740 --> 00:05:53.780
This practice is already in widespread use by companies like Google and Apple to protect

00:05:53.779 --> 00:05:59.429
users data and continues to gain more adoption including within the blockchain space.

